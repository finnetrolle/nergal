"""Clarification agent for disambiguating user queries.

This agent analyzes user messages for ambiguity and generates
clarifying questions when needed.
"""

import json
import logging
from typing import Any

from nergal.dialog.agents.base_specialized import BaseSpecializedAgent
from nergal.dialog.constants import CLARIFICATION_KEYWORDS
from nergal.dialog.base import AgentResult, AgentType
from nergal.dialog.styles import StyleType
from nergal.llm import BaseLLMProvider, LLMMessage, MessageRole

logger = logging.getLogger(__name__)


class ClarificationAgent(BaseSpecializedAgent):
    """Agent for clarifying ambiguous user queries.
    
    This agent determines if a user's question is ambiguous and
    generates appropriate clarifying questions to improve understanding.
    
    The agent can:
    - Detect ambiguity in queries
    - Generate clarifying questions
    - Interpret user responses to clarifications
    """
    
    # Configure base class behavior
    _keywords = CLARIFICATION_KEYWORDS
    _context_keys = []
    _base_confidence = 0.2
    _keyword_boost = 0.15
    _context_boost = 0.3
    
    def __init__(
        self,
        llm_provider: BaseLLMProvider,
        style_type: StyleType = StyleType.DEFAULT,
        max_clarifications: int = 2,
    ) -> None:
        """Initialize the clarification agent.
        
        Args:
            llm_provider: LLM provider for generating responses.
            style_type: Response style to use.
            max_clarifications: Maximum clarifying questions per query.
        """
        super().__init__(llm_provider, style_type)
        self._max_clarifications = max_clarifications
    
    @property
    def agent_type(self) -> AgentType:
        """Return the type of this agent."""
        return AgentType.CLARIFICATION
    
    @property
    def system_prompt(self) -> str:
        """Return the system prompt for this agent."""
        return """–¢—ã ‚Äî –∞–≥–µ–Ω—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å,
–Ω—É–∂–Ω–æ –ª–∏ —É—Ç–æ—á–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å —É—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å.

–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –∑–∞–ø—Ä–æ—Å –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏:
- –ù–µ—è—Å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏–ª–∏ –ø–æ–Ω—è—Ç–∏—è
- –ù–µ—Å–∫–æ–ª—å–∫–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–π
- –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –≤–∞–∂–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- –°–ª–∏—à–∫–æ–º –æ–±—â–∏–π –≤–æ–ø—Ä–æ—Å

–ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —è—Å–µ–Ω –∏ –æ–¥–Ω–æ–∑–Ω–∞—á–µ–Ω ‚Äî –≤–µ—Ä–Ω–∏ JSON —Å clarification_needed: false.
–ï—Å–ª–∏ –Ω—É–∂–Ω–æ —É—Ç–æ—á–Ω–µ–Ω–∏–µ ‚Äî –≤–µ—Ä–Ω–∏ JSON —Å clarification_needed: true –∏ –≤–æ–ø—Ä–æ—Å–æ–º.

–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{
    "clarification_needed": true/false,
    "reason": "–ø—Ä–∏—á–∏–Ω–∞ –ø–æ—á–µ–º—É –Ω—É–∂–Ω–æ/–Ω–µ –Ω—É–∂–Ω–æ —É—Ç–æ—á–Ω–µ–Ω–∏–µ",
    "question": "—É—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å –Ω–∞ —Ä—É—Å—Å–∫–æ–º",
    "options": ["–≤–∞—Ä–∏–∞–Ω—Ç1", "–≤–∞—Ä–∏–∞–Ω—Ç2"] // –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
}"""

    async def can_handle(self, message: str, context: dict[str, Any]) -> float:
        """Determine if this agent should handle the message.
        
        Higher confidence for messages that seem ambiguous or
        when clarification has been requested.
        
        Args:
            message: User message to analyze.
            context: Current dialog context.
            
        Returns:
            Confidence score (0.0 to 1.0).
        """
        # Check if clarification was explicitly requested
        if context.get("needs_clarification"):
            return 0.95
        
        # Use base class keyword matching
        return await super().can_handle(message, context)

    async def process(
        self,
        message: str,
        context: dict[str, Any],
        history: list[LLMMessage],
    ) -> AgentResult:
        """Process the message by checking for ambiguity.
        
        Args:
            message: User message to process.
            context: Current dialog context.
            history: Message history.
            
        Returns:
            AgentResult with clarification if needed.
        """
        # Analyze message for ambiguity
        analysis, tokens_used = await self._analyze_message(message, context)
        
        if not analysis.get("clarification_needed", False):
            return AgentResult(
                response="–ó–∞–ø—Ä–æ—Å –ø–æ–Ω—è—Ç–µ–Ω, —É—Ç–æ—á–Ω–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.",
                agent_type=self.agent_type,
                confidence=0.5,
                metadata={
                    "clarification_needed": False,
                    "reason": analysis.get("reason", ""),
                },
                tokens_used=tokens_used,
            )
        
        # Format clarification response
        response = self._format_clarification_response(analysis)
        
        return AgentResult(
            response=response,
            agent_type=self.agent_type,
            confidence=0.9,
            metadata={
                "clarification_needed": True,
                "question": analysis.get("question", ""),
                "options": analysis.get("options", []),
                "reason": analysis.get("reason", ""),
            },
            tokens_used=tokens_used,
        )
    
    async def _analyze_message(
        self,
        message: str,
        context: dict[str, Any],
    ) -> tuple[dict[str, Any], int | None]:
        """Analyze message for ambiguity.
        
        Args:
            message: User message to analyze.
            context: Dialog context.
            
        Returns:
            Tuple of (analysis result dictionary, tokens used or None).
        """
        prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∑–∞–ø—Ä–æ—Å –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏.

–ó–∞–ø—Ä–æ—Å: {message}

–û–ø—Ä–µ–¥–µ–ª–∏:
1. –Ø—Å–µ–Ω –ª–∏ –∑–∞–ø—Ä–æ—Å –∏–ª–∏ —Ç—Ä–µ–±—É–µ—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è
2. –ï—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç ‚Äî —Å—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π —É—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å
3. –ï—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ ‚Äî –ø—Ä–µ–¥–ª–æ–∂–∏ –≤–∞—Ä–∏–∞–Ω—Ç—ã –æ—Ç–≤–µ—Ç–∞"""

        messages = [
            LLMMessage(role=MessageRole.SYSTEM, content=self.system_prompt),
            LLMMessage(role=MessageRole.USER, content=prompt),
        ]
        
        response = await self.llm_provider.generate(messages, max_tokens=300)
        
        tokens_used = None
        if response.usage:
            tokens_used = response.usage.get("total_tokens") or (
                response.usage.get("prompt_tokens", 0) + response.usage.get("completion_tokens", 0)
            )
        
        # Parse JSON response
        try:
            start = response.content.find("{")
            end = response.content.rfind("}") + 1
            if start != -1 and end > start:
                return json.loads(response.content[start:end]), tokens_used
        except json.JSONDecodeError:
            pass
        
        # Default: no clarification needed
        return {
            "clarification_needed": False,
            "reason": "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å",
        }, tokens_used
    
    def _format_clarification_response(self, analysis: dict[str, Any]) -> str:
        """Format clarification response for user.
        
        Args:
            analysis: Analysis result dictionary.
            
        Returns:
            Formatted response string.
        """
        question = analysis.get("question", "–£—Ç–æ—á–Ω–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–∞—à –∑–∞–ø—Ä–æ—Å.")
        options = analysis.get("options", [])
        
        response = f"ü§î {question}"
        
        if options:
            response += "\n\n–í–∞—Ä–∏–∞–Ω—Ç—ã:\n"
            for i, option in enumerate(options[:4], 1):
                response += f"{i}. {option}\n"
        
        return response
